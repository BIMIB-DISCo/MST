##############################################################################
###
### MST
###
### Reconstruction Data
###
##############################################################################
### Copyright (c) 2015-2018, The TRONCO Team (www.troncopackage.org)
### email: tronco@disco.unimib.it
### All rights reserved. This program and the accompanying materials
### are made available under the terms of the GNU GPL v3.0
### which accompanies this distribution
##############################################################################


library(parallel)

load('RData/dataset.missing.impute.RData')

## Setting of the experiments

seed = 12345

available.cores = detectCores()

if (available.cores > 40) {
    cores = 40
} else if (available.cores > 1) {
    cores = available.cores - 1
} else {
    cores = 1
}


## Function to compute the results at each step

getStats <- function(true_matrix, inferred_matrix) {

    ##print(true_matrix)
    ##print(inferred_matrix)

    ## compute the statistics
    tp = 0
    tn = 0
    fp = 0
    fn = 0
    for (i in 1:nrow(inferred_matrix)) {
        for (j in i:ncol(inferred_matrix)) {
            if (i != j) {
                if (true_matrix[i, j] == 0 && inferred_matrix[i, j] == 0) {
                    tn = tn + 1
                } else if (true_matrix[i, j] == 0 && inferred_matrix[i, j] == 1) {
                    fp = fp + 1
                } else if (true_matrix[i, j] == 1 && inferred_matrix[i, j] == 0) {
                    fn = fn + 1
                } else if (true_matrix[i, j] == 1 && inferred_matrix[i, j] == 1) {
                    tp = tp + 1
                }
            }
        }
    }

    ## compute the statistics
    accuracy = (tp + tn)/(tp + tn + fp + fn)
    sensitivity = (tp)/(tp + fn)
    specificity = (tn)/(fp + tn)
    hamming_distance = fp + fn

    ## check for NAs generated by 0/0 computations
    ## this can happen for sensitivity or specificity
    if(is.na(sensitivity)) {
        sensitivity = 0
    }
    if(is.na(specificity)) {
        specificity = 0
    }

    ## return the results
    results_values = list(accuracy = accuracy,
                          sensitivity = sensitivity,
                          specificity = specificity,
                          hamming_distance = hamming_distance)
    return(results_values)
}


## Perform the reconstructions
run.reconstructions <- function(dataset,
                                true.tree,
                                epos, eneg,
                                debug = FALSE,
                                seed = NA,
                                pass.error.rates = TRUE) {

    if (!pass.error.rates) {
        epos = 0
        eneg = 0
    }

    results = NULL

    silent = !debug

    if (is.na(seed)) {
        seed = as.integer(runif(1) * 10000)
    }

    ## Create a TRONCO object of the dataset
    data = import.genotypes(dataset)

    ## Create the igraph structure
    net = empty.graph(colnames(as.genotypes(data)), num = 6)
    categoric.dataset = data.frame(apply(as.genotypes(data), 2, factor))
    for (name in colnames(categoric.dataset)) {
        levels(categoric.dataset[[name]]) = c(0, 1)
    }

    ## Performs the reconstructions with CAPRI loglik, aic and bic
    res = NULL
    res = tronco.capri(data,
                       regularization = c("bic"),
                       epos = epos,
                       eneg = eneg,
                       silent = silent,
                       boot.seed = seed)
    adj.matrix.capri.bic =
        as.adj.matrix(res, model = "capri_bic")[['capri_bic']]
    result = getStats(true.tree, adj.matrix.capri.bic)
    amat(net[[1]]) = adj.matrix.capri.bic
    score = bnlearn::score(net[[1]], categoric.dataset, type = 'loglik')
    capri = list(adj = adj.matrix.capri.bic,
                 score = score,
                 result = result)
    results[["capri"]] = capri

    ## Performs the reconstructions with CAPRESE
    res = NULL
    res = tronco.caprese(data,
                         epos = epos,
                         eneg = eneg,
                         silent = silent)
    adj.matrix.caprese = as.adj.matrix(res, model = "caprese")[['caprese']]
    result = getStats(true.tree, adj.matrix.caprese)
    amat(net[[2]]) = adj.matrix.caprese
    score = bnlearn::score(net[[2]], categoric.dataset, type = 'loglik')
    caprese = list(adj = adj.matrix.caprese,
                   score = score,
                   result = result)
    results[["caprese"]] = caprese

    ## Performs the reconstructions with Edmonds no_reg, loglik, aic and bic
    res = NULL
    res = tronco.edmonds(data,
                         regularization = c("no_reg"),
                         score = c('pmi'),
                         epos = epos,
                         eneg = eneg,
                         silent = silent,
                         boot.seed = seed)
    adj.matrix.edmonds.pmi.no.reg =
        as.adj.matrix(res, model = "edmonds_no_reg_pmi")[['edmonds_no_reg_pmi']]
    result = getStats(true.tree, adj.matrix.edmonds.pmi.no.reg)
    amat(net[[3]]) = adj.matrix.edmonds.pmi.no.reg
    score = bnlearn::score(net[[3]], categoric.dataset, type = 'loglik')
    edmonds = list(adj = adj.matrix.edmonds.pmi.no.reg,
                   score = score,
                   result = result)
    results[["edmonds"]] = edmonds

    ## Performs the reconstructions with Gabow
    res = NULL
    res = tronco.gabow(data,
                       regularization = c("no_reg"),
                       score = c('pmi'),
                       epos = epos,
                       eneg = eneg,
                       silent = silent,
                       boot.seed = seed)
    adj.matrix.gabow.pmi.no.reg =
        as.adj.matrix(res, model = "gabow_no_reg_pmi")[['gabow_no_reg_pmi']]
    result = getStats(true.tree, adj.matrix.gabow.pmi.no.reg)
    amat(net[[4]]) = adj.matrix.gabow.pmi.no.reg
    score = bnlearn::score(net[[4]], categoric.dataset, type = 'loglik')
    gabow = list(pmi.no.reg.adj = adj.matrix.gabow.pmi.no.reg,
                 score = score,
                 result = result)
    results[["gabow"]] = gabow

    ## Performs the reconstructions with Chow Liu loglik, aic and bic
    res = NULL
    res = tronco.chowliu(data,
                         regularization = c("loglik"),
                         epos = epos,
                         eneg = eneg,
                         silent = silent,
                         boot.seed = seed)
    adj.matrix.chowliu.loglik =
        as.adj.matrix(res, model = "chow_liu_loglik")[['chow_liu_loglik']]
    result = getStats(true.tree, adj.matrix.chowliu.loglik)
    amat(net[[5]]) = adj.matrix.chowliu.loglik
    score = bnlearn::score(net[[5]], categoric.dataset, type = 'loglik')
    chowliu = list(loglik.adj = adj.matrix.chowliu.loglik,
                   score = score,
                   result = result)
    results[["chowliu"]] = chowliu

    ## Performs the reconstructions with Prim no_reg, loglik, aic and bic
    res = NULL
    res = tronco.prim(data,
                      regularization = c("no_reg"),
                      epos = epos,
                      eneg = eneg,
                      silent = silent,
                      boot.seed = seed)
    adj.matrix.prim.no.reg =
        as.adj.matrix(res, model = "prim_no_reg")[['prim_no_reg']]
    result = getStats(true.tree, adj.matrix.prim.no.reg)
    amat(net[[6]]) = adj.matrix.prim.no.reg
    score = bnlearn::score(net[[6]], categoric.dataset, type = 'loglik')
    prim = list(adj = adj.matrix.prim.no.reg,
                score = score,
                result = result)
    results[["prim"]] = prim

    return(results)
}


expand.input <- function(datasets, seed, cores, pass.error.rates = TRUE) {
    cat('Using', cores, 'cores via "parallel" \n')
    cl = makeCluster(cores, outfile = '')
    clusterEvalQ(cl, library(TRONCO))
    clusterEvalQ(cl, library(bnlearn))
    clusterExport(cl, 'run.reconstructions')
    clusterExport(cl, 'getStats')
    clusterSetRNGStream(cl, iseed = seed)

    for(i in 1:nrow(datasets)) {
        for (j in 1:ncol(datasets)) {
            cat((((i - 1) * ncol(datasets)) + j) , '/',
                nrow(datasets) * ncol(datasets), '\n')
            single.experiment = datasets[[i, j]]
            results = parLapply(cl,
                                single.experiment,
                                function(x, pass.error.rates) {
                                    run.reconstructions(x$dataset,
                                                        x$true_tree,
                                                        x$epos,
                                                        x$eneg,
                                                        pass.error.rates = pass.error.rates)
                                },
                                pass.error.rates = pass.error.rates)
            for (k in 1:length(results)) {
                datasets[[i, j]][[k]]$reconstructions = results[[k]]
            }
        }
    }
    stopCluster(cl)
    return(datasets)
}


experiment.missing.data =
    expand.input(dataset.missing.impute, seed = seed, cores = cores)
save(experiment.missing.data,
     file = "RData/experiment.missing.data.RData")

### end of file -- reconstruct.data.R

